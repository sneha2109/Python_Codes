{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9064df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download requirements\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for file name to convert to csv\n",
    "my_file = input(\"File to convert to csv:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bde844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert file to csv\n",
    "base = os.path.splitext(my_file)[0]\n",
    "new_name = base + '.csv'\n",
    "os.rename(my_file, new_name)\n",
    "#print(base + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read converted file into dataframe\n",
    "df = pd.read_csv(new_name)\n",
    "#df1 = pd.read_csv(\"mtsamples_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257efc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two csv files\n",
    "df1 = pd.read_csv(\"cities.csv\")\n",
    "df_1 = pd.DataFrame(df)\n",
    "df_2 = pd.DataFrame(df1)\n",
    "#df_1.eq(df_2)\n",
    "#if schemas match, do nothing.  If they do not, prompt for whether or not to merge the two dataframes\n",
    "if df_1.equals(df_2) == True:\n",
    "    print(\"Columns match\")\n",
    "else:\n",
    "    my_response = input(\"Schemas are not the same. Would you like to convert the file to appropriate format?:\")\n",
    "    if (my_response == \"Yes\"):\n",
    "        df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "    #df3 = df_1.merge(df_2, how = 'inner', indicator=False)\n",
    "        print(df3)\n",
    "    else:\n",
    "        print(\"No changes made\")\n",
    "\n",
    "#print(df_1.compare(df_2))\n",
    "#print(df_1 == df_2)\n",
    "#df_2.reset_index(drop=True, inplace=True)\n",
    "#if df_1 == df_2:\n",
    " #   print(\"true\")\n",
    "#else:\n",
    " #   print(\"false\")\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a40faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store database file as dataframe and look at schema\n",
    "import sqlite3\n",
    "con = sqlite3.connect('NoteStore.sqlite')\n",
    "df3 = pd.read_sql_query('select * from sqlite_master', con)\n",
    "df_1 = pd.DataFrame(df3)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "#print(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef92e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare 2 different schemas - one database and one csv\n",
    "# store database file as dataframe \n",
    "import sqlite3\n",
    "con = sqlite3.connect('NoteStore.sqlite')\n",
    "df3 = pd.read_sql_query('select * from sqlite_master', con)\n",
    "df_1 = pd.DataFrame(df3)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "\n",
    "# store csv file as dataframe\n",
    "df2 = pd.read_csv(\"cities.csv\")\n",
    "df_2 = pd.DataFrame(df2)\n",
    "pd.io.json.build_table_schema(df_2)\n",
    "\n",
    "# combine the two dataframes - option if prompts will not be needed\n",
    "df3 = pd.concat([df_1,df_2],axis=0, ignore_index=True)\n",
    "pd.io.json.build_table_schema(df3)\n",
    "#print(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fa53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at 2 schemas\n",
    "#df1 = pd.read_csv(\"cities.csv\")\n",
    "#df_1 = pd.DataFrame(df1)\n",
    "#pd.io.json.build_table_schema(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv(\"mtsamples.csv\")\n",
    "#df_2 = pd.DataFrame(df2)\n",
    "#pd.io.json.build_table_schema(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3907618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two similar dataframes - csv - into one dataframe\n",
    "df1 = pd.read_csv(\"cities.csv\")\n",
    "df_1 = pd.DataFrame(df1)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "df2 = pd.read_csv(\"mtsamples.csv\")\n",
    "df_2 = pd.DataFrame(df2)\n",
    "pd.io.json.build_table_schema(df_2)\n",
    "#df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "df3 = pd.concat([df_1,df_2],axis=0, ignore_index=True)\n",
    "pd.io.json.build_table_schema(df3)\n",
    "#print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25894445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json file into csv\n",
    "# convert json file into dataframe\n",
    "pdObj = pd.read_json('movies.json', orient='index')\n",
    "\n",
    "# convert dataframe to csv \n",
    "csvData = pdObj.to_csv('streaming.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare one dataframe to another\n",
    "#df1 = pd.read_csv(\"cities.csv\")\n",
    "#df_1 = pd.DataFrame(df)\n",
    "#df_2 = pd.DataFrame(df1)\n",
    "#df_1.eq(df_2)\n",
    "#if df_1.equals(df_2) == True:\n",
    " #   print(\"Columns match\")\n",
    "#else:\n",
    " #   my_response = input(\"Schemas are not the same. Would you like to convert for file to appropriate format?:\")\n",
    "  #  if (my_response == \"Yes\"):\n",
    "  #      df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "    #df3 = df_1.merge(df_2, how = 'inner', indicator=False)\n",
    "  #      print(df3)\n",
    "  #  else:\n",
    "  #      print(\"No changes made\")\n",
    "\n",
    "#print(df_1.compare(df_2))\n",
    "#print(df_1 == df_2)\n",
    "#df_2.reset_index(drop=True, inplace=True)\n",
    "#if df_1 == df_2:\n",
    " #   print(\"true\")\n",
    "#else:\n",
    " #   print(\"false\")\n",
    "#print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install data model\n",
    "!pip install simpletransformers --use-feature=2020-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df3[['transcription', 'medical_specialty']]\n",
    "data = data.drop(data[data['transcription'].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "\n",
    "def clean_text(text): \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    newtext = ''.join([w for w in text if not w.isdigit()]) \n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    newtext = newtext.lower()\n",
    "    newtext = REPLACE_BY_SPACE_RE.sub('', newtext)\n",
    "    STOPWORDS = set(get_stop_words('en'))\n",
    "    words = newtext.split(\" \")\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in STOPWORDS:\n",
    "            words.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    newtext = ' '.join(map(str, words))\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['medical_specialty'].value_counts()\n",
    "others = [k for k,v in counts.items() if v<100]\n",
    "for each_spec in others:\n",
    "    data.loc[data['medical_specialty']==each_spec, 'medical_specialty'] = 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'transcription'] = [clean_text(x) if x == x else \"\" for x in df['transcription'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy, pandas, torch, sklearn\n",
    "!python3 -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(analyzer='word')\n",
    "feature_space=vectorizer.fit_transform(list(df['transcription']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(feature_space.todense(), columns=vectorizer.get_feature_names())\n",
    "new_df=pd.concat([df, count_vect_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b239c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=new_df.loc[:, 'aa':]\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "new_df[\"medical_specialty_code\"] = lb_make.fit_transform(new_df[\"medical_specialty\"])\n",
    "\n",
    "Y=new_df['medical_specialty_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([X, Y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d03016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    \n",
    "    # reading the csv and defining predictor and output columns\n",
    "    def __init__(self):\n",
    "    \n",
    "        # store the input and output features\n",
    "        self.X = data.values[:,:-1] \n",
    "        self.y = data.values[:,-1] \n",
    "    \n",
    "        # ensure all data is numerical - type(float)\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = self.y.astype('float32')\n",
    "    \n",
    "    # number of rows in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X[index], self.y[index]]\n",
    "\n",
    "    # split into train and testset - using `random_split`\n",
    "    def get_splits(self, split_ratio = 0.2):\n",
    "        test_size = round(split_ratio * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    \n",
    "    # reading the csv and defining predictor and output columns\n",
    "    def __init__(self):\n",
    "    \n",
    "        # store the input and output features\n",
    "        self.X = data.values[:,:-1] \n",
    "        self.y = data.values[:,-1] \n",
    "    \n",
    "        # ensure all data is numerical - type(float)\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = self.y.astype('float32')\n",
    "    \n",
    "    # number of rows in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X[index], self.y[index]]\n",
    "\n",
    "    # split into train and testset - using `random_split`\n",
    "    def get_splits(self, split_ratio = 0.2):\n",
    "        test_size = round(split_ratio * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CSVDataset()\n",
    "# get the train and test split\n",
    "train, test = dataset.get_splits()\n",
    "# prepare dataloaders - \n",
    "train_dl = DataLoader(train, batch_size = 100, shuffle = True)\n",
    "test_dl = DataLoader(test, batch_size= 100, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "model = myNeuralNetwork(X.shape[1]) # 2 because we only have 2 input features\n",
    "# define the number of epochs\n",
    "epochs = 30\n",
    "# define the optimizer - Adam\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # go through all the batches generated by dataloader\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(inputs)\n",
    "        loss = criterion(yhat, targets.type(torch.LongTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb98b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Evaluate the model\n",
    "predictions, actuals = list(), list()\n",
    "# loop over all batches in test set\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # pass input to the model\n",
    "    y_pred = model(inputs) \n",
    "    # retrieve the numpy array\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    # pick the index of the highest values\n",
    "    res = np.argmax(y_pred, axis = 1) \n",
    "    \n",
    "    # actual output\n",
    "    actual = targets.numpy()\n",
    "    actual = actual.reshape(len(actual), 1)\n",
    "    \n",
    "    # store the values in respective lists\n",
    "    predictions.append(list(res))\n",
    "    actuals.append(list(actual))\n",
    "    \n",
    "actuals = [val for sublist in np.vstack(list(chain(*actuals))) for val in sublist]\n",
    "predictions = [val for sublist in np.vstack(list(chain(*predictions))) for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b42102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating how good the model is!\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy is\" , accuracy_score(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd8fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
