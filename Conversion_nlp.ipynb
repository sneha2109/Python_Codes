{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download requirements\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for file name to convert to csv\n",
    "my_file = input(\"File to convert to csv:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67354b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert file to csv\n",
    "base = os.path.splitext(my_file)[0]\n",
    "new_name = base + '.csv'\n",
    "os.rename(my_file, new_name)\n",
    "#print(base + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ee215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read converted file into dataframe\n",
    "df = pd.read_csv(new_name)\n",
    "#df1 = pd.read_csv(\"mtsamples_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two csv files\n",
    "df1 = pd.read_csv(\"cities.csv\")\n",
    "df_1 = pd.DataFrame(df)\n",
    "df_2 = pd.DataFrame(df1)\n",
    "#df_1.eq(df_2)\n",
    "#if schemas match, do nothing.  If they do not, prompt for whether or not to merge the two dataframes\n",
    "if df_1.equals(df_2) == True:\n",
    "    print(\"Columns match\")\n",
    "else:\n",
    "    my_response = input(\"Schemas are not the same. Would you like to convert the file to appropriate format?:\")\n",
    "    if (my_response == \"Yes\"):\n",
    "        df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "    #df3 = df_1.merge(df_2, how = 'inner', indicator=False)\n",
    "        print(df3)\n",
    "    else:\n",
    "        print(\"No changes made\")\n",
    "\n",
    "#print(df_1.compare(df_2))\n",
    "#print(df_1 == df_2)\n",
    "#df_2.reset_index(drop=True, inplace=True)\n",
    "#if df_1 == df_2:\n",
    " #   print(\"true\")\n",
    "#else:\n",
    " #   print(\"false\")\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store database file as dataframe and look at schema\n",
    "import sqlite3\n",
    "con = sqlite3.connect('NoteStore.sqlite')\n",
    "df3 = pd.read_sql_query('select * from sqlite_master', con)\n",
    "df_1 = pd.DataFrame(df3)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "#print(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17995c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare 2 different schemas - one database and one csv\n",
    "# store database file as dataframe \n",
    "import sqlite3\n",
    "con = sqlite3.connect('NoteStore.sqlite')\n",
    "df3 = pd.read_sql_query('select * from sqlite_master', con)\n",
    "df_1 = pd.DataFrame(df3)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "\n",
    "# store csv file as dataframe\n",
    "df2 = pd.read_csv(\"cities.csv\")\n",
    "df_2 = pd.DataFrame(df2)\n",
    "pd.io.json.build_table_schema(df_2)\n",
    "\n",
    "# combine the two dataframes - option if prompts will not be needed\n",
    "df3 = pd.concat([df_1,df_2],axis=0, ignore_index=True)\n",
    "pd.io.json.build_table_schema(df3)\n",
    "#print(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c326b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at 2 schemas\n",
    "#df1 = pd.read_csv(\"cities.csv\")\n",
    "#df_1 = pd.DataFrame(df1)\n",
    "#pd.io.json.build_table_schema(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv(\"mtsamples.csv\")\n",
    "#df_2 = pd.DataFrame(df2)\n",
    "#pd.io.json.build_table_schema(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two similar dataframes - csv - into one dataframe\n",
    "df1 = pd.read_csv(\"cities.csv\")\n",
    "df_1 = pd.DataFrame(df1)\n",
    "pd.io.json.build_table_schema(df_1)\n",
    "df2 = pd.read_csv(\"mtsamples.csv\")\n",
    "df_2 = pd.DataFrame(df2)\n",
    "pd.io.json.build_table_schema(df_2)\n",
    "#df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "df3 = pd.concat([df_1,df_2],axis=0, ignore_index=True)\n",
    "pd.io.json.build_table_schema(df3)\n",
    "#print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000079a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json file into csv\n",
    "# convert json file into dataframe\n",
    "pdObj = pd.read_json('movies.json', orient='index')\n",
    "\n",
    "# convert dataframe to csv \n",
    "csvData = pdObj.to_csv('streaming.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare one dataframe to another\n",
    "#df1 = pd.read_csv(\"cities.csv\")\n",
    "#df_1 = pd.DataFrame(df)\n",
    "#df_2 = pd.DataFrame(df1)\n",
    "#df_1.eq(df_2)\n",
    "#if df_1.equals(df_2) == True:\n",
    " #   print(\"Columns match\")\n",
    "#else:\n",
    " #   my_response = input(\"Schemas are not the same. Would you like to convert for file to appropriate format?:\")\n",
    "  #  if (my_response == \"Yes\"):\n",
    "  #      df3 = pd.concat([df_1,df_2]).drop_duplicates(keep=False)\n",
    "    #df3 = df_1.merge(df_2, how = 'inner', indicator=False)\n",
    "  #      print(df3)\n",
    "  #  else:\n",
    "  #      print(\"No changes made\")\n",
    "\n",
    "#print(df_1.compare(df_2))\n",
    "#print(df_1 == df_2)\n",
    "#df_2.reset_index(drop=True, inplace=True)\n",
    "#if df_1 == df_2:\n",
    " #   print(\"true\")\n",
    "#else:\n",
    " #   print(\"false\")\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef220eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install data model\n",
    "!pip install simpletransformers --use-feature=2020-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bf6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['transcription', 'medical_specialty']]\n",
    "data = data.drop(data[data['transcription'].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16822db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "\n",
    "def clean_text(text): \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    newtext = ''.join([w for w in text if not w.isdigit()]) \n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    newtext = newtext.lower()\n",
    "    newtext = REPLACE_BY_SPACE_RE.sub('', newtext)\n",
    "    STOPWORDS = set(get_stop_words('en'))\n",
    "    words = newtext.split(\" \")\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in STOPWORDS:\n",
    "            words.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    newtext = ' '.join(map(str, words))\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['medical_specialty'].value_counts()\n",
    "others = [k for k,v in counts.items() if v<100]\n",
    "for each_spec in others:\n",
    "    data.loc[data['medical_specialty']==each_spec, 'medical_specialty'] = 'others' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "data = data.rename(index = str, columns = {'transcription': 'text'})\n",
    "data.loc[:, 'text'] = [clean_text(x) for x in data['text'].values]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(data['medical_specialty'])\n",
    "data.loc[:, 'class_label'] = le.transform(data['medical_specialty'])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data[\"tokens\"] = data[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def BOW(data):\n",
    "    df_temp = data.copy(deep = True)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit(df_temp['text'])\n",
    "    list_corpus = df_temp[\"text\"].tolist()\n",
    "    list_labels = df_temp[\"class_label\"].tolist()\n",
    "    X = count_vectorizer.transform(list_corpus)\n",
    "    return X, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf1(data, ngrams = 1):\n",
    "    df_temp = data.copy(deep = True)\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngrams))\n",
    "    tfidf_vectorizer.fit(df_temp['text'])\n",
    "    list_corpus = df_temp[\"text\"].tolist()\n",
    "    list_labels = df_temp[\"class_label\"].tolist()\n",
    "    X = tfidf_vectorizer.transform(list_corpus)\n",
    "    return X, list_labels\n",
    "\n",
    "def tfidf2(data, ngrams = 2):\n",
    "    df_temp = data.copy(deep = True)\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngrams))\n",
    "    tfidf_vectorizer.fit(df_temp['text'])\n",
    "    list_corpus = df_temp[\"text\"].tolist()\n",
    "    list_labels = df_temp[\"class_label\"].tolist()\n",
    "    X = tfidf_vectorizer.transform(list_corpus)\n",
    "    return X, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def w2v(data):\n",
    "    \n",
    "    df_temp = data.copy(deep = True)\n",
    "    \n",
    "    embeddings = get_word2vec_embeddings(word2vec, df_temp)\n",
    "    list_labels = df_temp[\"class_label\"].tolist()\n",
    "    \n",
    "    return embeddings, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow, y_bow = BOW(data)\n",
    "X_tfidf1, y_tfidf1 = tfidf1(data)\n",
    "X_tfidf2, y_tfidf2 = tfidf2(data)\n",
    "X_w2v, y_w2v = w2v(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y_bow, test_size=0.2, random_state=1)\n",
    "X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1 = train_test_split(X_tfidf1, y_tfidf1, test_size=0.2, random_state=1)\n",
    "X_train_tfidf2, X_test_tfidf2, y_train_tfidf2, y_test_tfidf2 = train_test_split(X_tfidf2, y_tfidf2, test_size=0.2, random_state=1)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    precision = precision_score(y_test, y_predicted, average='weighted')             \n",
    "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def training_logreg(X_train_log, X_test_log, y_train_log, y_test_log, preproc):\n",
    "    \n",
    "    clf = LogisticRegression(solver = 'saga', multi_class = 'multinomial', n_jobs = -1, random_state = 1)\n",
    "    \n",
    "    clf.fit(X_train_log, y_train_log)\n",
    "    \n",
    "    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "    \n",
    "    y_pred = clf.predict(X_test_log)\n",
    "    \n",
    "    f1 = f1_score(y_pred, y_test_log, average = 'weighted')\n",
    "    pres = precision_score(y_pred, y_test_log, average = 'weighted')\n",
    "    rec = recall_score(y_pred, y_test_log, average = 'weighted')\n",
    "    acc = accuracy_score(y_pred, y_test_log)\n",
    "    \n",
    "    res = res.append({'Preprocessing': preproc, 'Model': f'Logistic Regression', 'Precision': pres, \n",
    "                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_result_logreg = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "full_result_logreg = full_result_logreg.append(training_logreg(X_train_bow, X_test_bow, y_train_bow, y_test_bow, \"bow\"), ignore_index = True)\n",
    "full_result_logreg = full_result_logreg.append(training_logreg(X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1, \"tfidf1\"), ignore_index = True)\n",
    "full_result_logreg = full_result_logreg.append(training_logreg(X_train_tfidf2, X_test_tfidf2, y_train_tfidf2, y_test_tfidf2, \"tfidf2\"), ignore_index = True)\n",
    "full_result_logreg = full_result_logreg.append(training_logreg(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v, \"w2v\"), ignore_index = True)\n",
    "full_result_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def training_naive(X_train_naive, X_test_naive, y_train_naive, y_test_naive, preproc):\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_naive, y_train_naive)\n",
    "\n",
    "    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "    \n",
    "    y_pred = clf.predict(X_test_naive)\n",
    "    \n",
    "    f1 = f1_score(y_pred, y_test_naive, average = 'weighted')\n",
    "    pres = precision_score(y_pred, y_test_naive, average = 'weighted')\n",
    "    rec = recall_score(y_pred, y_test_naive, average = 'weighted')\n",
    "    acc = accuracy_score(y_pred, y_test_naive)\n",
    "    \n",
    "    res = res.append({'Preprocessing': preproc, 'Model': 'Naive Bayes', 'Precision': pres, \n",
    "                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "naive_under_sample = RandomUnderSampler(sampling_strategy = 'majority', random_state = 1)\n",
    "X_bow_rus, y_bow_rus = naive_under_sample.fit_resample(X_bow, y_bow)\n",
    "X_tfidf1_rus, y_tfidf1_rus = naive_under_sample.fit_resample(X_tfidf1, y_tfidf1)\n",
    "X_w2v_rus, y_w2v_rus = naive_under_sample.fit_resample(X_w2v, y_w2v)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_w2v_rus = scaler.fit_transform(X_w2v_rus)\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow_rus, y_bow_rus, test_size=0.2, random_state=1)\n",
    "X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1 = train_test_split(X_tfidf1_rus, y_tfidf1_rus, test_size=0.2, random_state=1)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v_rus, y_w2v_rus, test_size=0.2, random_state=1)\n",
    "\n",
    "naive_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "naive_result = naive_result.append(training_naive(X_train_bow, X_test_bow, y_train_bow, y_test_bow, 'bow'), ignore_index = True)\n",
    "naive_result = naive_result.append(training_naive(X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1, 'tfidf1'), ignore_index = True)\n",
    "naive_result = naive_result.append(training_naive(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v, 'w2v'), ignore_index = True)\n",
    "naive_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install lightgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e24c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def training_lgbm(X_train_lgbm, X_test_lgbm, y_train_lgbm, y_test_lgbm, preproc,\n",
    "                 n = 120, depth = 4, child = 2, bag = 0.9, feature = 0.9, l1 = 1, l = 0.01):\n",
    "    \n",
    "    if preproc == \"w2v\":\n",
    "        d_train = lgb.Dataset(X_train_lgbm, label=y_train_lgbm)\n",
    "    else:\n",
    "        d_train = lgb.Dataset(X_train_lgbm.astype(np.float32), label=y_train_lgbm)\n",
    "\n",
    "    early_stop = 500\n",
    "    verbose_eval = False\n",
    "    num_rounds = n\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': len(set(y_train_lgbm)),\n",
    "        'boosting': 'gbdt', \n",
    "        'metric': 'multi_logloss',\n",
    "        'max_depth': depth, \n",
    "        'max_bin': 22, \n",
    "        'bagging_fraction': bag, \n",
    "        'feature_fraction': feature, \n",
    "        'min_child_samples': child, \n",
    "        'min_child_weight': 1, \n",
    "        'learning_rate': l,\n",
    "        'verbosity': -1, \n",
    "        'data_random_seed': 17,\n",
    "        'lambda_l1': l1}\n",
    "\n",
    "    model = lgb.train(params, train_set = d_train, num_boost_round = num_rounds)\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test_lgbm.astype(np.float32), num_iteration=model.best_iteration)\n",
    "    y_pred = [np.argmax(x) for x in y_pred_proba]\n",
    "    \n",
    "    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "        \n",
    "    f1 = f1_score(y_pred, y_test_lgbm, average = 'weighted')\n",
    "    pres = precision_score(y_pred, y_test_lgbm, average = 'weighted')\n",
    "    rec = recall_score(y_pred, y_test_lgbm, average = 'weighted')\n",
    "    acc = accuracy_score(y_pred, y_test_lgbm)\n",
    "    \n",
    "    res = res.append({'Preprocessing': preproc, 'Model': 'LightGBM', 'Precision': pres, \n",
    "                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def training_svm(X_train_svm, X_test_svm, y_train_svm, y_test_svm, preproc):\n",
    "\n",
    "    model = SVC()\n",
    "    \n",
    "    model.fit(X_train_svm, y_train_svm)\n",
    "    \n",
    "    y_pred = model.predict(X_test_svm)\n",
    "    \n",
    "    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "        \n",
    "    f1 = f1_score(y_pred, y_test_svm, average = 'weighted')\n",
    "    pres = precision_score(y_pred, y_test_svm, average = 'weighted')\n",
    "    rec = recall_score(y_pred, y_test_svm, average = 'weighted')\n",
    "    acc = accuracy_score(y_pred, y_test_svm)\n",
    "    \n",
    "    res = res.append({'Preprocessing': preproc, 'Model': 'LightGBM', 'Precision': pres, \n",
    "                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn_over_sample = ADASYN(sampling_strategy='minority')\n",
    "X_bow_a, y_bow_a = adasyn_over_sample.fit_resample(X_bow, y_bow)\n",
    "X_tfidf1_a, y_tfidf1_a = adasyn_over_sample.fit_resample(X_tfidf1, y_tfidf1)\n",
    "X_w2v_a, y_w2v_a = adasyn_over_sample.fit_resample(X_w2v, y_w2v)\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow_a, y_bow_a, test_size=0.2, random_state=1)\n",
    "X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1 = train_test_split(X_tfidf1_a, y_tfidf1_a, test_size=0.2, random_state=1)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v_a, y_w2v_a, test_size=0.2, random_state=1)\n",
    "\n",
    "adasyn_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n",
    "adasyn_result = adasyn_result.append(training_svm(X_train_bow, X_test_bow, y_train_bow, y_test_bow, 'bow'), ignore_index = True)\n",
    "adasyn_result = adasyn_result.append(training_svm(X_train_tfidf1, X_test_tfidf1, y_train_tfidf1, y_test_tfidf1, 'tfidf1'), ignore_index = True)\n",
    "adasyn_result = adasyn_result.append(training_svm(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v, 'w2v'), ignore_index = True)\n",
    "adasyn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a803e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=1)\n",
    "best_model = LogisticRegression(solver = 'saga', multi_class = 'multinomial', n_jobs = -1, random_state = 1)\n",
    "best_model.fit(X_train_w2v, y_train_w2v)\n",
    "y_test_pred = best_model.predict(X_test_w2v)\n",
    "y_test = y_test_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=10, rotation = 90)\n",
    "    plt.yticks(tick_marks, classes, fontsize=10)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a927298",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data.medical_specialty.unique()\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "plot = plot_confusion_matrix(cm, classes=classes, normalize=False, \n",
    "                             title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classes = data.medical_specialty.unique()\n",
    "print(classification_report(y_test,y_test_pred,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d45ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
